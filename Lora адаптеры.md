**LoRA (Low-Rank Adaptation)** — это метод для эффективной тонкой настройки больших языковых моделей.  
**Суть:**

1. **Минимум изменений:** Вместо дообучения всех параметров модели (миллиарды) LoRA добавляет к ним небольшие "адаптеры" — низкоранговые матрицы, которые учатся решать конкретную задачу.
   
2. Адаптер представляет из себя 2 матрицы: первая - (emb_dim * rank), вторая  - (rank * emb_dim), при обучении любой эмбеддинг проходит через первый линйный слой FFN и паралельно через 2 матрицы LoRA, затем результаты складываются. Например у нас есть LLM у которой emb_dim 8192, и мы хотим дообучить LoRa адаптер на нём с размером 32, значит первый линйный слой FFN занимает 8192 * 8192 = 67 108 864 параметров на один слой, а размер LoRa адаптера будет 8192 * 32 + 32 * 8192 = 524 288 параметров на один слой, что в 128 раз меньше, таким образом мы обучаем на 2 порядка меньше параметров, что очень сокращает требования к ресурсам, однако эти 2 lora матрицы не будут такими же точными как оригинальная матрица весов слоя из за низкого ранга, но их точности будет достаточно для дообучения модели
	
3. **Экономия ресурсов:** Адаптеры содержат на порядки меньше параметров, что снижает затраты на вычисления и память.
	
4. **Гибкость:** Основная модель остаётся неизменной, а адаптеры можно быстро переобучать или менять под разные задачи (например, под специфичный стиль текста или домен).
	

**Пример применения:** Настройка LLM для медицинских диагнозов или юридических документов без полного переобучения модели.

в библиотеке PEFT обычно происходит дообучение LoRa адаптеров, и можно добавить их буквально на любой lenear слой, но обычно добавляют на feed forward слои