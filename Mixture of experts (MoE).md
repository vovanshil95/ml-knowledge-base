Mixture of Experts (MoE) — это архитектура нейронных сетей, которая делит большую модель на несколько специализированных подсетей — «экспертов», каждый из которых обучен решать свою конкретную подзадачу. Для каждого входного примера активируется только небольшое количество подходящих экспертов, что позволяет значительно повысить вычислительную эффективность и масштабируемость модели без пропорционального увеличения затрат ресурсов.

В LLM есть полносвязные слои
на каждом слое можно сделать свой роутер + n экспертов
также для LLM задать количество экспертов и количество tok-k экспертов, которые будут активироваться за 1 проход слоя на токене
при каждом проходе через FFN наш роутер получает входной эмбединг и отдает скаляр для каждого эксперта, (насколько данная тема (входной эмбеддинг) будет подходить для каждого эксперта), затем по этим значениям выбераются top-k экспертов и уже через каждый из этих top-k экспертов проходит эмбеддинг и выходы суммируются через взвешанную сумму, веса являются как раз этими скалярам из роутера.
Роутер представляет из себя линейный слой (d_model * n_experts) + softmax, на вход берёт эмбеды
один эксперт представляет из себя Linear(d_model, d_ff) -> f_activation -> Linear(d_ff, d_model), d_ff - размер скрытого слоя эксперта,
обычно = 4 * d_model.
Также есть возможность добавить экспертов, которые будут активироваться во всех случаях независимо от роутера, так и сделали в deepseek
также не стоит забывать, что нужен специальный лосс, который будет штрафовать за то, что какие то эксперты слишком часто работают, а какие то слишком редко, чтобы каждый эксперт работал +- одинаковое количество раз при приходе 