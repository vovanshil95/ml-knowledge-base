
Pretrain LLM это первая стадия обучения LLM она самая долгая и ресурсоёмкая, обычно для этого этапа собирают 1-20 триллионов токенов, она состоит из следующих этапов:

1. получение корпуса текстов из кучи документов
2. чистка корпуса претрейна
3. склеивание документов через "end of sentence" токен
4. разрезание контекста на блоки одинакового размера например 4096, размер блока должен совпадать с максимальным окном контекста модели
(Если в 1 блоке 2 документа или более то модель может получать контекст  из прошлых документов в текущий документ это неправильно ведь документы не связаны, чтобы этого избежать используют маску внимания (это матрица где 1 дайменшин - откуда мы берём контекст, а второй - куда), эта матрица умножается на саму матрицу внимания, полученную на блоке внимания. Чтобы занулить контекст из других документов мы зануляем все элементы маски, для которых i и j соответсвют номерам токенов из разных документов.
5. сдвиг сентенса вправо на 1 токен для получения меток 
6. получения эмбеддингов последнего слоя для каждого токена
7. создание предсказаний с помощью lm head на каждом для каждого токена и вычисление ошибки между предсказаниями и выходными токенами.
**Полезные советы**
- Использовать косинусный weighy decay
- Использовать warmap-steps 500 - 2000 (1%-5% от всех шагов)
- Общий батч состоит из 4M токенов
- Использовать 1 эпоху, были исследования, в них увеличение эпох до 2 дало прирост < 0.5