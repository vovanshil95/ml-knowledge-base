Prompt-tuning, P-tuning, Prefix-tuning и P-tuning v2  — это разновидности эффективной и экономной настройки больших языковых моделей, которые фокусируются на обучении небольшого набора дополнительных параметров (например, специальных токенов или префиксов) при заморозке весов основной модели. Их общая суть — улучшить поведение модели на конкретных задачах, добавляя или оптимизируя  prompt, без необходимости переобучения  весов модели. Такой подход снижает требования к вычислительным ресурсам и памяти, ускоряет настройку и позволяет быстро адаптировать модели под новые сценарии с минимальными затратами.

Тут подробно рассказывается о каждом методое:

- Prompt-tuning - добавлеем после промпта k - псевдо токенов (эмбедов,  которые инициализируются случайно), затем обучаем именно эти эмбеды, а веса модели замораживаем, главное преимущество - вычислительная эффективность ведь нужно бучить всего $d_{emb} * k$ параметров, однаком недостаток в том, что на средних моделях работает некачественно.

- P-tuning - похож на  Prompt-tuning, но мы получаем "мягкие эмбединги" не просто изначально, а получаем их из промпта с помощью prompt-encoder, которым может быть (LSTM + MLP), на этапе обучения обучается сам prompt-encoder, и количество параметров равно количеству параметров prompt-encoder'а. Также имеет возможность встраивать эмбеды в "Шаблон" шаблон это текст в котором "мягкие эмбеды" вшиваются в сам интсрукционный промпт например `[X] is [P1] and [P2] [Y]` -> `[Emb(X)] [Emb(is)] [Soft Emb(P1)] [Emb(and)] [Soft Emb(P2)] [Emb(Y_position)]` тут `X` это входной текст, `P1` и `P2` это и есть места в которые вставляются "мягкие эмбеды" `y` - выход. Также имеет мело параметров для обучения, и лучше работает с моделями среднего размера.
  
- Prefix tuning - как Prompt-tuning но добавляются не "мягкие эмбединги" а "мягкая матрица K" и "мягкая матрица V" длины n, причём не на этапе промпта а на каждом блоке внимания, причём на каждом слое используется одни и те же матрицы K и V, при обучении обучаются эти матрицы, количество параметров   $2 * d_{kv} * n_{embs}$, также имеет мело параметров для обучения, но из за того, что влияет на каждом блоке внимания напрямую, обучение может сильнее повлиять на генерацию и быть эффективнее.

- P-tuning v2 - то же самое что и Prefix tuning, однако теперь не одна "мягкая матрица K" и "мягкая матрица V" длины n на каждый слой, а отдельные "мягкие  матрицы k и v" для каждого слоя, но есть второй режим работы в котором эти матрицы получаются с помощью того же prompt-encoder'а, на вход которому подаются входной эмбед в блоке внимания, самый эффективный метод, количество параметров $2 * d_{kv} * n_{embs} * layers$, в случае обучения матриц, либо же каличество параметров энкодера в случае режима работы с энкодером.

Для работы не нужен сам промпт с инструкциями, кроме случая с p-tuning, тут он может быть предоставлен как шаблон. "Мягкие / сгенерированные эмбеды" в Prompt-tuning добавляются после основных эмбедов а в prefix-tuning и p-tuning v2 перед ними, в p-tuning в зависимости от шаблона