**Reinforcement Learning from Human Feedback (RLHF)** — это метод машинного обучения, который улучшает обучение с подкреплением (RL) за счёт использования обратной связи от человека вместо заранее прописанной функции вознаграждения. Вместо того чтобы жёстко определять, что считать «правильным» поведением, RLHF обучает специальную модель вознаграждения на основе оценки человеком различных вариантов действий или ответов агента. Затем эта модель вознаграждения служит для оптимизации поведения искусственного интеллекта через классические методы RL.

Этот подход особенно важен для сложных задач, где традиционные функции вознаграждения трудно формализовать, например, создание безопасного и полезного диалога, соответствующего человеческим ценностям и предпочтениям. RLHF помогает системам ИИ более точно «понимать» человеческие ожидания, снижать нежелательные ошибки и повышать качество взаимодействия с пользователем.

Тут будет показан метод proximal policy optimization (PPO) - стандартный подход реализации RLHF для языковых моделей.

-  PPO - proximal policy optimization метод которым можно реализовать reinfrcement learning, для дообучения больших языковых моделей
  PPO objective function  $L_{Total}=L^{CLIP}​​−c1​⋅L^{VF}​​ - β⋅L^{KL}​​+c2​⋅L^{Entropy}​​$
  $L^{CLIP}=E_t​[min(r_t​A_t​,clip(r_t​,1−ϵ,1+ϵ)A_t​)]$ - фактически 
  $r_t$ = вероятность предсказания токена новой политикой (новой LLM) / вероятность предсказания токена старой политикой (старой LLM)
  при чём это на происходит на текстах сгенерированных старой LLM
  $​A_t$ это Advantage насколько сгенерированный токен увеличивает оценку финальной награды вычисляется несколькими методами, на практике используется TD-ошибка (Temporal Difference Error)
  $A_t​=r_t​+γV(s_{t+1}​)−V(s_t​)$
  $r_t​=\frac{r_{total​}}{T}$
  $V(s_t)$ - оценка состояния модели предсказания reword текущего состояния
   $r_t​=\frac{r_{total​}}{T}$
  clipping нужен чтобы модель не изменяла вероятности относительно старой модели больше чем на $ϵ$ это нужно для того чтобы модель не слишком отходила от своего стандартной и не переобучалась, но нужно заметить, что этот clipping не работает, в случае если новая модель работает намного хуже старой (при A > 0 и r < 1 - $ϵ$  или  A < 0 и r > $1+ϵ$ ), но работает если модель лучше старой (при A < 0 и r < 1 - $ϵ$  или  A > 0 и r > $1+ϵ$ )
  $L^{VF}=E_t​[(V_θ​(st​)−R_t​)^2]$ - mse от предсказания дисконтированной целевой награды
  дисконтированная целевая награда $R_t​=∑_{k=0}^{T−t}​γ^k⋅r_{total}$  
  $r_{total}​$ — оценка всего текста от reward model, $γ$ — коэффициент дисконтирования,
  $T$ — длина текста. Например текст из 3 токенов: `[t1, t2, t3]`. 
  Оценка reward model: $r_{total}=0.8$.
  Тогда: 
  Для $t_1​: R_1=0.8⋅(1+γ+γ^2)R_1​=0.8⋅(1+γ+γ^2)$,
  Для $t_2​: R_2=0.8⋅(1+γ) R_2​=0.8⋅(1+γ)$,
  Для $t_3​: R_3=0.8⋅1 R_3​=0.8⋅1$
  Следующая компонента - $L^{KL}$ - средняя по токенам КЛ дивергенция между распределением вероятностей предсказаний токенов старой и новой модели - грубо говоря разница между старой и новой моделью нужна чтобы модель сильно не отходила от старой
  $L_{Entropy}=E_t​[−∑_aπθ​(a∣st​)log (πθ​(a∣st​))]$ - энтропия по распределению логитов модели усреднённая по каждому токену - нужна чтобы модель не сошлась к генерациям одного токена и взлому reward модели