
BERT (Bidirectional Encoder Representations from Transformers) — это языковая модель на основе архитектуры [[Архитектура трансформер|трансформера]], которая обучается учитывать контекст слова одновременно с обеих сторон (слева и справа), что позволяет глубже понимать смысл текста. BERT использует только encoder часть трансформера. Модель получает на вход токены и формирует контекстуальные векторные представления токенов, которые эффективно используются для разнообразных задач NLP после дообучения. BERT стал базой для множества современных решений в обработке естественного языка таких как

- Классификацию текста (например, тематическую классификацию, спам-фильтры, выявление признаков);
    
- Анализ тональности (определение эмоциональной окраски текста: позитивная, негативная, нейтральная);
    
- Сопоставление и проверку логической связности предложений (next sentence prediction);
    
- Поиск и выделение релевантных документов в базах знаний.

![[Pasted image 20250720161234.png]]

Предобучение происходит засчёт маскирования рандомных токенов и предсказания линйным слоем из обогащённого контекстом эмбединга этого токена, этот процесс называется mlm (Masked language modeling)
Дообучение происходит на классификацию происходит так же само, но там вместо замаскированного токена используется class-token, его принято ставить в начале последовательности (эксперименты показали, что такая позиция даёт лучшее качество, я думаю это связано с тем, что position encoding для первого токена всегда равен 0 не зависимо от длины последовательности)