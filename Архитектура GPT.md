
GPT (Generative Pre-trained Transformer) — это языковая модель на основе архитектуры [[Архитектура трансформер|трансформера]], разработанная компанией OpenAI. В отличие от [[Архитектура BERT предобучение дообучение|BERT]], GPT использует только декодерную часть [[Архитектура трансформер|трансформера]] и обучается в режиме автогенерации: модель предсказывает следующий токен на основе предыдущих, извлекая смысл из контекста слева (т.е. autoregressive, или авторегрессионный режим). Благодаря этому GPT способна последовательно генерировать осмысленный текст и поддерживать логику в длинных последовательностях.

GPT проходит две стадии обучения: сначала предобучение на больших объемах текстовых данных без надзора, затем — дообучение (fine-tuning) на конкретные задачи

. Благодаря этому она формирует богатые контекстуальные представления текста, делая её универсальным инструментом для различных задач в обработке естественного языка.

GPT легла в основу множества современных решений, среди которых:

- **Генерация связного и осмысленного текста** (статьи, посты, описания, сценарии, рекламные материалы).
    
- **Чат-боты и виртуальные ассистенты:** исполнение диалогов, ответы на вопросы, поддержка клиентов.
    
- **Перевод и перефразирование текстов**
    
- **Автоматическое суммирование и выделение главного** из больших текстов, документов и новостных заметок.
    
- **Поиск и генерация программного кода** (на основе описания задачи).
    
- **Анализ тональности и семантической близости** (например, для определения эмоций в тексте или сопоставления запросов и ответов).