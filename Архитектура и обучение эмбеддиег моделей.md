Эмбеддинг-модели, также называемые bi-encoder нужны для преобразования сложных, неструктурированных данных (в данном разделе текст) в числовые векторы, которые компьютер может эффективно анализировать и использовать. Это позволяет машинам "понимать" смысл слов и объектов, различать контекст и семантику, а также быстро и точно искать и классифицировать тексты.

Эмбеддинг модели это [[Архитектура BERT предобучение дообучение|Bert-like]] модели, которые используют как выход mean pulling всех эмбеддингов, иногда используют эмбеддинг класс-токена но реже. Они обучаются из обычных [[Архитектура BERT предобучение дообучение|BERT]] после стадии MLM методом контрастного обучения - разбивают датасет на пары примеров и размечают является ли пара похожей, далее обучают на парах штрафуют за близость векторов непохожей пары и за большое расстояние векторов похожей пары. Используют следующие лоссы
Triplet Loss:
`L = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)`
- `A` (Anchor) – исходный текст
- `P` (Positive) – похожий текст
- `N` (Negative) – непохожий текст
Contrastive Loss:
`loss = ‖u - v‖² если похожи и max(margin - ‖u - v‖, 0)² если не похожи`
 - `u`  и `v` это рандомная пара документов, которая может быть либо похожей либо нет