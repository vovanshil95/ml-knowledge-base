Архитектура состоит из двух частей: **кодировщика (энкодера)** и **декодировщика (декодера)**. Кодировщик преобразует входную последовательность в набор вложенных векторов с учётом контекста (через несколько слоев многоголового внимания и [[Механизмы positional encoding|позиционного кодирования]]). Декодировщик, принимая эти контекстные представления и частичную выходную последовательность, генерирует итоговый текст. Для передачи позиции элементов используется **позиционное кодирование**, поскольку трансформер не учитывает порядок слов напрямую.

Основные компоненты трансформера:

- **Multi-head self-attention** — несколько параллельных слоёв внимания, улучшающих способность модели захватывать разные аспекты контекста.
    
- **Feed-forward нейронные сети** — двухслойные сети после внимания для нелинейных преобразований.
    
- **Residual connections (пропускные связи)** и **layer normalization** — улучшают обучение и стабилизируют градиенты.

- **Embedding слой** — (линйный слой с размерностью (словарь токенов * размерность эмбеддингов)) преобразует входные токены (слова или субсловные единицы) в векторные представления (эмбеддинги), которые понимает модель.
  
- **Unembedding слой** (или выходной слой) (линйный слой с размерностью (размерность эмбеддингов * словарь токенов)) — проецирует внутренние представления модели (эмбеддинги) обратно в вероятности следующих токенов 
  
![[Pasted image 20250720155645.png]]
Архитектура трансформер (на картинке показаны блоки энкодера и декодера в рамках, на деле это последовательность из n блоков энкодера и декодера)

