Мы не можем просто взять и сделать инициализацию весов одинаковой $w  = N(0,1)$, ведь это может привести к взрыву или к затуханию градиентов, чем больше дисперсия, тем больше средние модули весов, тем больше вероятность взрыва, и чем меньше дисперсия, тем меньше модули весов, и тем больше вероятность затухания градиентов, также это зависит от количества входных  нейронов, чем их больше тем сумма выходов тоже больше, чтобы решить эту проблему, используют алгоритмы инициализации весов, в зависимости от количества входных нейронов
**Метод Ксавье Глоро**
	Метод основан на двух требованиях
	- Дисперсия выходов каждого слоя должна быть равна дисперсии его входов
	- Дисперсия градиентов по входам слоя должна быть равна дисперсии градиентов по его выходамыходам
	также на предположениях
	- Веса инициализированы **независимо**.
	- Веса и входные данные имеют **нулевое среднее**.
	- Активационная функция `f` приблизительно **линейна** в начале обучения и обладает свойством `f'(0) ≈ 1` (это верно для сигмоиды и tanh в окрестности 0, но не выполняется для ReLu подобных функций).
	чтобы удовлетворить этим требованиям, при заданных условиях, нужно чтобы `Var(W) = 1 / fan_in` при прямом распостранени, и `Var(W) = 1 / fan_out` при обратном распостранении, чтобы прийти к компромису, можно найти среднее из этих двух дисперсий как `Var(W) = 2 / (fan_in + fan_out)`, итак, мы можем выбрать распределение со средним  в 0 и дисперсией `Var(W) = 2 / (fan_in + fan_out)`, например это может быть нормальное распределение `W ~ N(0, sqrt(2 / (fan_in + fan_out)))` или равномерное распределение `W ~ U(-a, a)`, `a = sqrt(6 / (fan_in + fan_out))`
**He-инициализация (Kaiming Initialization)**
	этот метод является адаптацие метода Ксавье Глоро, но с учётом того что функция не относительно линейна и не симметрична в 0, этот метод заточен на функции подобные ReLu. Идея метода в том, что ReLu зануляет половину активаций, поэтому мы **просто** действуем как в методе Ксавье Глоро, но увеличиваем дисперсию в 2 раза. Для нормального распределения `W ~ N(0, sqrt(4 / (fan_in + fan_out)))`, для равномерного распределения `W ~ U(-a, a)`, `a = sqrt(12 / (fan_in + fan_out))`, однако экспертименты показали, что можно не искать двойное среднее размеров входов и выходов, а использовать двойной размер входа, это объясняется тем, что обратное распостранение адаптируется если при прямом проходе затухания не будет то и на обратном итого финальная формула `W ~ N(0, sqrt(2 / fan_in))` или `W ~ U(-a, a)`, `a = sqrt(6 / fan_in)` также моя интуиция говорит, что Relu симметрична по масштабу вперёд и назад если смотреть значения > 0, поэтому если прямой проход не взрывает и не затухает и не взрывает градиенты, то обратный тоже не должен, поэтому можно не учитывать `fan_out`, а значения < 0 
В LLM
На ембеддинг (token -> emb) слоях используется `N(0, 0.02)` или he инициализация.
На слоях FFN используется He-инициализация, тк там функции активации подобные ReLu.
На слоях внимания используется Метод Ксавье Глоро тк нет ReLu который делит выходы на 2, инициализация.
На анембеддинг (emb -> token) слоях используется `N(0, 0.006)` или `U(-0.002, 0.002)`.