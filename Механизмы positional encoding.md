
**Positional encoding** в трансформерах — это способ добавить информацию о порядке слов в последовательности, так как сама архитектура обрабатывает токены параллельно и не знает их позиции. Это позволяет модели учитывать структуру и смысл предложения, различая, например, the cat sees the dog" и "the dog sees the cat", несмотря на одинаковый набор слов.

#### **Тригонометрические функции**

PE(pos, 2i)   = sin(pos / 10000^(2i/d_model)) (это для четных измерений эмбеда)
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model)) (это для нечетных измерений эмбеда)
где 2i и 2i+1 это измерение эмбеда а pos это позиция эмбеда
таким образом каждое измерение эмбеда имеет свою частоту, тк какие-то смословые части  часто меняются а какие то медленно
таким же абсолютно образом работает и с изображениями однако с той оговоркой что у текста можно скзаать есть одна координата (порядковый номер слова в предложении), а у изображения 2 координаты (порядковый номер патча по вертикали и горизонтали), мы просто берём сумму энкодинга позиции по вертикали и по горизонтали ))))

#### **RoPE**

Этот механизм используется только на этапе внимания, что логичнее, мы используем матрицу разворота вектора на угол `θ`
Rθ = `[[cosθ, -sinθ],  [sinθ,  cosθ]]`
`θ = pos / 10000^(2i/d_head)`
далее мы умножаем эту матрицу на key и query в слое внимания
`q_rot = Rθ_q * q`

`k_rot = Rθ_k * k`
Точнее мы делим эти векторы `q` и `k` на подвектора по с размерностью 2 и их уже эти подвектора умножаем на матрицу разворота.
Видно что угол зависит от позиции токена а также от `i` тоесть позиции подвектора в векторе, это значит что чем больше расстояние между словами, тем больше и угол, а также, что угол изменяется быстрее для начала эмбеддинга и медленне для конца эмбеддинга, тоесть начало эмбеддинга изменяется быстрее и кодирует краткосрочные зависимости.
Это лучше чем просто синусоидальные эмбеды потому что
- инвариантны к сдвигу
- позици относительны
- имеет смысл вращения вектора, тоесть его длина вектора та же