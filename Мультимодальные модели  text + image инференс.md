**Мультимодальные модели** — это системы искусственного интеллекта, способные одновременно воспринимать и обрабатывать данные из разных источников, в данном случае — текст и изображения. Они создают общее представление информации, объединяя смысловое содержание текста с визуальными признаками изображения в одном векторном пространстве.

Их архитектура состоит из следующих этапов 

1. Получение эмбеддингов изображения с помощью [[Vision Transformer]]
  
2. Затем их можно объединить с текстом несколькими вариантами

	**2.1 Раннее объединение (Early Fusion)**
	- Визуальные эмбеддинги **конкатенируются с текстовыми эмбедингами**, образуя единый входной поток токенов.
	    
	- Затем вся последовательность поступает в языковую модель (например, [[Архитектура GPT|GPT]]) и обрабатывается как обычный текст, где патчи изображения воспринимаются как текстовые токены.
	    
	- Модель учится совместно понимать текст и изображение в одном пространстве, напрямую интегрируя обе модальности с самого начала обработки.
	
	**Особенности:**
	
	- Простая и интуитивная интеграция;
	    
	- Визуальные данные «обрабатываются как токены текста»
	    
	- Пример — модель LLaVA[](https://blog.deepschool.ru/llm/v-llm/)
	    
	- Требует больших данных для обучения, но обеспечивает тесное объединение модальностей.
	
	**2.2 Глубокое объединение через Cross-Attention с адаптерами (Deep Fusion)**
	- Эмбеддинги проходят через адаптеры — небольшие полносвязные слои (feed-forward), переводящие визуальное представление в близкое к текстовому пространство.
	    
	- В слоях языкового трансформера реализован механизм **cross-attention**, где **query** формируется из текста, а **key и value** — из визуальных эмбеддингов. Это позволяет текстовым эмбедингам выборочно получать и инкорпорировать информацию из изображения.
	    
	- В результате происходит асимметричное взаимодействие: текст обращается к визуальной информации, но визуальные эмбеддинги не изменяются с учётом текста и сохраняются как фиксированные.
	
	**Особенности:**
	
	- Позволяет сохранить предварительно обученное визуальное представление;
	    
	- Модальность визуального представления остаётся относительно независимой до этапа cross-attention;
	    
	- Эффективно обрабатывает изображения разных разрешений;
	    
	- Требует отдельного обучения cross-attention и адаптеров, что может требовать больших ресурсов;
	    
	- Более гибкий способ интеграции, акцент на «внимании» текста к картинке, а не на объединении токенов.**2.1 Раннее объединение (Early Fusion)**
	- Визуальные эмбеддинги **конкатенируются с текстовыми эмбедингами**, образуя единый входной поток токенов.
	    
	- Затем вся последовательность поступает в языковую модель (например, [[Архитектура GPT|GPT]]) и обрабатывается как обычный текст, где патчи изображения воспринимаются как текстовые токены.
	    
	- Модель учится совместно понимать текст и изображение в одном пространстве, напрямую интегрируя обе модальности с самого начала обработки.
	
	**Особенности:**
	
	- Простая и интуитивная интеграция;
	    
	- Визуальные данные «обрабатываются как токены текста»
	    
	- Пример — модель LLaVA[](https://blog.deepschool.ru/llm/v-llm/)￼
	    
	- Требует больших данных для обучения, но обеспечивает тесное объединение модальностей.
	
	**2.2 Глубокое объединение через Cross-Attention с адаптерами (Deep Fusion)**
	- Эмбеддинги проходят через адаптеры — небольшие полносвязные слои (feed-forward), переводящие визуальное представление в близкое к текстовому пространство.
	    
	- В слоях языкового трансформера реализован механизм **cross-attention**, где **query** формируется из текста, а **key и value** — из визуальных эмбеддингов. Это позволяет текстовым эмбедингам выборочно получать и инкорпорировать информацию из изображения.
	    
	- В результате происходит асимметричное взаимодействие: текст обращается к визуальной информации, но визуальные эмбеддинги не изменяются с учётом текста и сохраняются как фиксированные.
	
	**Особенности:**
	
	- Позволяет сохранить предварительно обученное визуальное представление;
	    
	- Модальность визуального представления остаётся относительно независимой до этапа cross-attention;
	    
	- Эффективно обрабатывает изображения разных разрешений;
	    
	- Требует отдельного обучения cross-attention и адаптеров, что может требовать больших ресурсов;
	    
	- Более гибкий способ интеграции, акцент на «внимании» текста к картинке, а не на объединении токенов.