
обе архитектуры это части [[Архитектура трансформер|трансоформера]].

[[Архитектура GPT|GPT]] это декодерная часть а [[Архитектура BERT предобучение дообучение|BERT]] это - энкодерная. Отличие в том что encoder не использует маску для ограничения получения контекста из будущих слов, а декодер использует эту маску. Таким образом из за этой маски при исполнении механизма внимания в Bert слово получае контексты из всех слов документа, а gpt только слов предшествующих данному

**Влияние на применение:**

- **BERT** лучше подходит для задач, требующих глубокого понимания и анализа текста, например, классификация, поиск, определение намерений, извлечение информации, где важен полный контекст.
    
- **GPT** эффективен там, где требуется генерация нового текста, ведение диалогов, создание креативных ответов и поддержка динамичных взаимодействий с пользователем.