Cуществует несколько типов паралелизма применимо к LLM, на стадии обучения, я буду указывать их по уровню частоты взаимодействия вычеслений между собой
1. cuda паралелизм (паралелизм внутри одной видеокарты между cuda ядрами, реализован в библиотеках cuBLAS/cuDNN и драйверах видеокарты), он паралелит тензорные перемножения
2.  Tensor паралелизм (TP) паралелит тензорные преобразования но уже между несколькими gpu, также может распаралелить применение функций активации к слою 
	1.  также рассмаотривают sequence паралелизм как подвид TP
3. pipeline паралелизм (PP) - размещение первых 8 слоёв модели на 1 gpu, вторых - на втором и так далее, пока вторые 8 слоёв обрабатывают предыдущий батч, первые уже делают следующий batch (принцип конвейера)
4.  Data паралелизм (обработка нескольких батчей нексколькими GPU)		существует несколько типов паралелизма применимо к LLM, на стадии обучения, я буду указывать их по уровню частоты взаимодействия вычеслений между собой
5. cuda паралелизм (паралелизм внутри одной видеокарты между cuda ядрами, реализован в библиотеках cuBLAS/cuDNN и драйверах видеокарты), он паралелит тензорные перемножения
6.  Tensor паралелизм (TP) паралелит тензорные преобразования но уже между несколькими gpu, также может распаралелить применение функций активации к слою 
	1.  также рассмаотривают sequence паралелизм как подвид TP
7. pipeline паралелизм (PP) - размещение первых 8 слоёв модели на 1 gpu, вторых - на втором и так далее, пока вторые 8 слоёв обрабатывают предыдущий батч, первые уже делают следующий batch (принцип конвейера)
8.  Data паралелизм (обработка нескольких батчей нексколькими GPU)