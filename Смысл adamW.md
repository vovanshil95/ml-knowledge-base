AdamW это улучшение классического [[Градиентный спуск|стохастического градиентного спуска]].
Сейчас этот подход - SOTA для опртимизаторов основанных на градиентном спуске.
Тут будет указан его алгоритм и смысл:

Для начало нужно разобраться в Adam
	Adam это улучшение классического SGD
	`θ_t = θ_{t-1} - α * (m_t_hat / (√v_t_hat + ε))
	рассмотрим по частям
	1. `θ_{t-1}` -  веса на прошлом шаге
	2. `- α * (m_t_hat / (√v_t_hat + ε))` `α` это learning rate
		Что же такое `v_t_hat`
		   это экспоненциальное скользящее среднее первого момента градиента, скользящее среднее значит, что оно учитывает все предыдущие значения, тоесть имеет некую энерцию вычисляется как 
		   `m_t = β1 * m_{t-1} + (1 - β1) * g_t` - Сырая оценка момента
		   `m_t_hat = m_t / (1 - β1^t)` - Смещённая оценка
		   `β1` коэффициент отвечающий за силу энерции/память, обычно 0.9
		   `g_t` просто градиент от функции потерь по весам также включающий регуляризацию `g_t = ∇L(θ) + λ * θ`
		   что значит смещённая оценка и зачем нам это смещение? а просто в начале `m_t` = 0, значит первоначально алгоритм будет вести себя так же как он бы себя вёл, если бы всё время гадиент был равен нулю, а это нетак, поэтому мы должны менее учитывать момент в первоначальных шагах и больше в последующих, например возьмём первый шаг `t=1`, 	`β1=0.9`, значит несмещённая оценка `m_1 = 0.9*0 + 0.1*g_1 = 0.1*g_1` - это очень мало, а смещённая - `m_1_hat = (0.1*g_1) / (1 - 0.9^1) = (0.1*g_1)/0.1 = g_1` - в самый раз для первого шага, ведь энерции и нет.
		   подытоживая нужно сказать, что `m_t_hat` просто градиент это и причём взятый с энерцией.
		Что же такое `m_t_hat`
		   `v_t = β2 * v_{t-1} + (1 - β2) * g_t^2` - Сырая оценка момента
		   `m_t_hat = m_t / (1 - β2^t)` - Смещённая оценка
		   тут всё абсолютно так же, однако вместо `g_t` тут `g_t^2` и это полностью меняет его смысл, `m_t_hat` это не градиент с энерцией а масштаб этого градиента с энерцией, тоесть если у нас слишком большие градиенты то `m_t_hat` будет их стабилизировать, чтобы все веса обновлялись не слишком сильно если гигантский градиент и не слишком слабо если он супер маленький, это также можно назвать дисперсией градиента, что похоже на масштаб,
		   также стоит заметить, что `β2` обычно 0.999, это потому, что масштаб должен быть менее изменчив со временем, чем направление
Теперь AdamW
	Пробема в том, что регуляризация через `g_t` попадает в `v_t` и `m_t`, но ведь она никак не связана с самими градиентами, и не должна влиять на них и причём тк существует экспоненциальное сглаживание, эффект регуляризации только накапливается с каждым шагом.
	Поэтому было решено убрать регуляризацию из подсчёта градиента а добавлять его в формулу отдельно теперь формула выглядит так же как в Adam но
	`g_t = ∇L(θ)`
	`θ_t = θ_{t-1} - α * m_hat / (√v_hat + ε) - α * λ * θ_{t-1}`
	итого вся формула
	`g_t = ∇L(θ)`
	`m_t = β1*m_{t-1} + (1-β1)*g_t`
	`v_t = β2*v_{t-1} + (1-β2)*(g_t)**2`
	`m_hat = m_t / (1 - β1**t)`
	`v_hat = v_t / (1 - β2**t)
	`θ_t = θ_{t-1} - α * m_hat / (√v_hat + ε) - α * λ * θ_{t-1}`
	всё)