lr-decay и warmup это практики изменения learning rate в процессе обучения LLM для улучшения сходимости:

**Смысл warmup**
- при AdamW в котором происходит деление на `sqrt(v_t)` может быть взрыв градиентов тк он очень мал, тк это начало обучения и он может близок к нулю тк не имеет инерции ещё
- Модель может сразу же уйти в плохую область безвовратно (коллапс), тк вначале её легче туда увести ведь веса случпйны
- имперические данные показали уменьшение perplexity, а также уменьшение количества коллапсов
- Для 100K шагов: 500-2000 обычно используют шагов прогрева
**Смысл lr-decay**
- помогает тонкой настройке модели в конце, улучшает сходимость
- уменьшает вероятность перепрыгивания оптимума когда он уже виден
- убирает осцеляции лоссов в конце
Также можно взять `lr = 3 * 10^-4` для 7b модели и `lr = 1.5 * 10^-4` для 13B для 1T токенов
Есть эвристика от meta `peak_lr = 3e-4 * sqrt(1B / n_params)`