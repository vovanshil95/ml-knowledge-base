Токенизаторы — это важнейший компонент обработки текста в современных языковых моделях, который разбивает текст на более мелкие части — токены. Благодаря токенизации текст переводится в числовой формат (ID токенов), что позволяет моделям эффективно обучаться и генерировать язык
Правильный выбор и обучение токенизатора критичны, поскольку он определяет, как именно текст разложится на части, влияет на качество понимания и генерации, а также на размер словаря и производительность модели
Существуют разные подходы к токенизации:


- **Byte Pair Encoding (BPE) / Byte-Level BPE (BBPE)**
	самый лучший способ, который работает с самими байтами и готов обработать любой байт, объединяет в один токен 2 байта если они частво встречаются в тексте, не нуждается в `<unk>` токенах, потому что любой байт и его последовательность ему знакома.
	1. В начале у нас есть 256 токенов (2^8)
	2. Затем мы проходимся по всему обучающему корпусу и размечаем все байты по токенам.
	3. Затем уже идём второй раз и находим самую частую пару токенов идущих подряд и объединяем её в один токен.
	4. Перереразмечаем те- **Byte Pair Encoding (BPE) / Byte-Level BPE (BBPE)**
	самый лучший способ, который работает с самими байтами и готов обработать любой байт, объединяет в один токен 2 байта если они частво встречаются в тексте, не нуждается в `<unk>` токенах, потому что любой байт и его последовательность ему знакома.
	5. В начале у нас есть 256 токенов (2^8)
	6. Затем мы проходимся по всему обучающему корпусу и размечаем все байты по токенам.
	7. Затем уже идём второй раз и находим самую частую пару токенов идущих подряд и объединяем её в один токен.
	8. Перереразмечаем текст с учётом нового токена
	9. Возвращаемся к пункту 2 снова по тексту
	10. делаем так, пока количество токенов не дойдёт до vocab_size
	Но сложность слишком большая около O(N_tokens * vocab_size), чтобы это оптимизировать существует подход
	- Делим весь корпус на слова и токенизирем слова
	- Делаем так, чтобы в каждом токене была ссылка на все слова в котором он есть
	- Проходимся по всем словам и ищем самые частые пары, записываем их в словарь типа `{пара: частота}`, у словаря должен быть функционал поиска максимума за O(1)
	- Ищем пару с максимальной частотой
	- Идём во все слова, где есть оба токена из пары, склеиваем токены в один, формируем новые пары (новые - только пары с новым токеном, тк после склейки остальные пары остались прежними, кроме пары которая сама образовала новый токен - её больше нет), эти новые пары получаем путём добавления к новому токену токена слева и справа, если они есть
	- Считаем частоты новых пар, добавляем их в словарь,
	- Удаляем из словаря старую пару (из которой появился новый токен)
	- Удаляем для первого и второго старого токена пары, все их ссылки на слова, где они идут парой друг за другом, и добавляем ссылки на эти слова в новый токен
	- возвращаемся на момент поиска пары с максимальной частото
	- делаем так, пока количество токенов не дойдёт до vocab_size
	также важно сказать, про пробелы, мы их воспринимем как обычные байты, за исключением того, что мы добавляем пробел в начале каждого предложения и мы разделяем предствляем каждое слово как " слово" а не "слово", чтобы был пробел в начале и модель его видела.
	Такой подход с разделением на слова имеет минус - в том, что нельзя запихать несколько слов в один токен.
	Однако это можно решить если не разделять текст на слова а воспринимать его одним целым текстом, где пробел это просто байт, а для каждого токена добавлять ссылки не на слова в тексте в которых он состоит (тк у нас нет разделения на слова), а например для каждого токена хранить ссылки на все упомянания токена в тексте, также хранить текст из токенов как двусвязный список чтобы можно было изменять склеивать элементы.
	- также нужно не забыть добавить пробел в начало каждой строки
	- обучать модель на всем тренировочном корпусе или если это физически невозможно, то на максимально большой выборке чательно отобранной и репрезентативной
	​￼Как устроен unicode, и utf-8 (нужно понимать чтобы разбираться в токенизации на уровне байтов)
		Мы кодируем все символы либо последовательностью из 1 - 4 байтов
		самые частые символы (анкглийский алфавит), самые частые знаки препинания и спец символы в байте
		Также кодировка совместима с ascii, тоесть основные 128 символов ascii имеют такую же кодировку в юникод, и это и является первым байтом.
		но ведь в 1 байтом можно закодировать больше символов?
		да но первый бит отвечает за то является ли символ который кодирует данный байт "однобайтовым"
		Если в символе 1 байт то начало -  "0"
		если 2 байта, то "110"
		если 3 байта, то "1110"
		если 4 байта, то "11110"
		"10" - если байт не первый для символа, а продолжения
​￼- **Character-level Tokenization******
	токенизует 1 символ как 1 токен, OOV не будет, но последовательности будут очень длинными, он используется в s2t моделях 
​￼- **Word-level Tokenization/ Whitespace Tokenization**
		токенизует 1 слово как 1 токен, но в тексте так много уникальных слов, что либо будет много токенов OOV либо гигантский словарь  
​￼- **WordPiece Tokenization**
	тут сначала проводится предварительная токенизация на основе правил, затем уже берётся склеивание как в BPE, но просто через `freq(X, Y)` а через `freq(X, Y) / (freq(X) * freq(Y))` (это правдоподобие униграмной языковой модели)
​￼- **Unigram Language Modeling**
	напишу позжекст с учётом нового токена
	1. Возвращаемся к пункту 2 снова по тексту
	2. делаем так, пока количество токенов не дойдёт до vocab_size
	Но сложность слишком большая около O(N_tokens * vocab_size), чтобы это оптимизировать существует подход
	- Делим весь корпус на слова и токенизирем слова
	- Делаем так, чтобы в каждом токене была ссылка на все слова в котором он есть
	- Проходимся по всем словам и ищем самые частые пары, записываем их в словарь типа `{пара: частота}`, у словаря должен быть функционал поиска максимума за O(1)
	- Ищем пару с максимальной частотой
	- Идём во все слова, где есть оба токена из пары, склеиваем токены в один, формируем новые пары (новые - только пары с новым токеном, тк после склейки остальные пары остались прежними, кроме пары которая сама образовала новый токен - её больше нет), эти новые пары получаем путём добавления к новому токену токена слева и справа, если они есть
	- Считаем частоты новых пар, добавляем их в словарь,
	- Удаляем из словаря старую пару (из которой появился новый токен)
	- Удаляем для первого и второго старого токена пары, все их ссылки на слова, где они идут парой друг за другом, и добавляем ссылки на эти слова в новый токен
	- возвращаемся на момент поиска пары с максимальной частото
	- делаем так, пока количество токенов не дойдёт до vocab_size
	также важно сказать, про пробелы, мы их воспринимем как обычные байты, за исключением того, что мы добавляем пробел в начале каждого предложения и мы разделяем предствляем каждое слово как " слово" а не "слово", чтобы был пробел в начале и модель его видела.
	Такой подход с разделением на слова имеет минус - в том, что нельзя запихать несколько слов в один токен.
	Однако это можно решить если не разделять текст на слова а воспринимать его одним целым текстом, где пробел это просто байт, а для каждого токена добавлять ссылки не на слова в тексте в которых он состоит (тк у нас нет разделения на слова), а например для каждого токена хранить ссылки на все упомянания токена в тексте, также хранить текст из токенов как двусвязный список чтобы можно было изменять склеивать элементы.
	- также нужно не забыть добавить пробел в начало каждой строки
	- обучать модель на всем тренировочном корпусе или если это физически невозможно, то на максимально большой выборке чательно отобранной и репрезентативной
	Как устроен unicode, и utf-8 (нужно понимать чтобы разбираться в токенизации на уровне байтов)
		Мы кодируем все символы либо последовательностью из 1 - 4 байтов
		самые частые символы (анкглийский алфавит), самые частые знаки препинания и спец символы в байте
		Также кодировка совместима с ascii, тоесть основные 128 символов ascii имеют такую же кодировку в юникод, и это и является первым байтом.
		но ведь в 1 байтом можно закодировать больше символов?
		да но первый бит отвечает за то является ли символ который кодирует данный байт "однобайтовым"
		Если в символе 1 байт то начало -  "0"
		если 2 байта, то "110"
		если 3 байта, то "1110"
		если 4 байта, то "11110"
		"10" - если байт не первый для символа, а продолжения
- **Character-level Tokenization******
	токенизует 1 символ как 1 токен, OOV не будет, но последовательности будут очень длинными, он используется в s2t моделях 
- **Word-level Tokenization/ Whitespace Tokenization**
		токенизует 1 слово как 1 токен, но в тексте так много уникальных слов, что либо будет много токенов OOV либо гигантский словарь  
- **WordPiece Tokenization**
	тут сначала проводится предварительная токенизация на основе правил, затем уже берётся склеивание как в BPE, но просто через `freq(X, Y)` а через `freq(X, Y) / (freq(X) * freq(Y))` (это правдоподобие униграмной языковой модели)
- **Unigram Language Modeling**
	напишу позже