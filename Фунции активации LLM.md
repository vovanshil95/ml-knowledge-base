`RELU = x * (x > 0)`
Самая простая для ввода нелинейности

`SILU(x) = x * sigmoid(x)`, также называют `SWISH`
Эта функция гладкая, немонотонная, позволяет пропускать часть отрицательных значений, улучшая градиентное распространение и обучение. Она даёт более плавный переход, чем ReLU, и часто приводит к более стабильному и быстрому обучению.

`GELU(x) = x * Φ(x)` где  `Φ(x)` - функция для нормального распределения (1, 0)
Функция активации, приближённо задаваемая как GELU(x)=x⋅Φ(x)GELU(x)=x⋅Φ(x), где Φ(x)Φ(x) — функция распределения нормального закона (CDF). Она взвешивает вход с вероятностью, насколько значение кажется положительным, обеспечивая очень гладкую и статистически мотивированную нелинейность. GELU часто используется в трансформерах и моделях NLP, улучшая обучение по сравнению с ReLU и SiLU

`SWIGLU(x) = w2(Silu( w1(x) ) * v(x) )` не совсем функция активации а скорее весь слой

Задача такого слоя — динамически «маскировать» или масштабировать одну часть входа другой при помощи гладкой нелинейности SiLU, что помогает модели лучше контролировать поток информации и улучшает выразительную способность по сравнению с простыми активациями. `SWIGLU` - SOTA подход