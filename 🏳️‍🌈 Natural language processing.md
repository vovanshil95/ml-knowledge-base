#### **Классические подходы**

[[Как можно численно представить текстовые данные?]]

[[TF-IDF]]

[[Оценка перевода или дополнения текста]]


#### **Transformer-like архитектуры**

[[Архитектура трансформер]]

[[Архитектура BERT предобучение дообучение]]

[[Архитектура и обучение эмбеддиег моделей]]

[[Механизмы positional encoding]]

[[Архитектура GPT]]

[[Отличие BERT от GPT]]

[[Механизмы внимания и их оптимизации]]

[[Mixture of experts (MoE)]]

[[Мультимодальные модели  text + image инференс]]

[[Фунции активации LLM]]

[[Новые подходы в Llama-3]]

[[Новые подходы в QWEN, QwQ]]

[[Представления чисел и алгоритмы квантизации]]


#### **Обучение LLM**

[[Инициализация весов LLM]]

[[Pretrain LLM стратегии]]

[[Стратегии обучения токенизаторов]]

[[Смысл adamW]]

[[Gradient checkpointing и back propogation]]

[[Паралельные вычисления в LLM]]

[[Смысл lr-decay и warmup]]

[[Reinforcement learning on human feedback]]

[[Lora адаптеры]]

[[Мультимодальные модели text + image обучение]]

[[Prompt-tuning, P-tuning, Prefix-tuning, P-tuning v2]]

#### **Инференс LLM**

Fake reasoning problem

Function/tool calling

ReAct фреймворк

MCP (Model Context Protocol)

A2A (Agent to Agent) протокол

RAG (Retrieval-Augmented Generation) идея

CAG (Cache Augmented Generation)

Хранение KV кэша в памяти

Бибилитотеки для быстрого инференса

#### **Интеллектуальный поиск**

RAG - SOTA подход

HNSW (FAISS)

BM25 (ELASTICSARCH)

Векторные базы данных

Метрики качества поиска и ранжирования